import pandas as pd
from datetime import datetime
import time
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import urllib3
import os
import logging
import requests
import json
from config_shared import PRIX_MOYEN_PAR_COLLECTION, SEUIL_BONNE_AFFAIRE, SEUIL_TRES_BONNE_AFFAIRE

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth

# On importe notre bo√Æte √† outils de scrapers
import scrapers

# --- CONFIGURATION GLOBALE ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

CONFIG_SITES = {
    "Amazon": { "type": "amazon", "use_selenium": True },
    "Lego": { "type": "standard", "selecteur": '[data-test="product-price"]', "use_selenium": False },
    "Auchan": { "type": "standard", "selecteur": ".product-price", "use_selenium": False },
    "Leclerc": { "type": "standard", "selecteur": ".egToM .visually-hidden", "use_selenium": False },
    "Carrefour": { "type": "carrefour", "selecteur": { "euros": ".product-price__content.c-text--size-m", "centimes": ".product-price__content.c-text--size-s" }, "use_selenium": True },
    # Ajoutez d'autres sites ici au besoin
}
FICHIER_EXCEL = "prix_lego.xlsx"
FICHIER_CONFIG_EXCEL = 'config_sets.xlsx'
EMAIL_ADRESSE = os.getenv('GMAIL_ADDRESS')
EMAIL_MOT_DE_PASSE = os.getenv('GMAIL_APP_PASSWORD')
EMAIL_DESTINATAIRE = os.getenv('MAIL_DESTINATAIRE')

# --- FONCTIONS UTILITAIRES ---

def charger_configuration_sets_df(fichier_config):
    """Lit simplement le fichier de configuration Excel et retourne un DataFrame."""
    try:
        df = pd.read_excel(fichier_config, dtype=str)
        df.fillna('', inplace=True)
        return df
    except Exception as e:
        logging.error(f"Erreur lors de la lecture de '{fichier_config}': {e}")
        return None

def regrouper_taches_par_site(df_config):
    """Transforme le DataFrame de configuration en un dictionnaire de t√¢ches group√©es par site."""
    taches_par_site = {}
    for index, row in df_config.iterrows():
        set_id = row['ID_Set']
        nom_set = row['Nom_Set']
        for site_nom, site_config in CONFIG_SITES.items():
            colonne_url = f"URL_{site_nom}"
            if colonne_url in row and row[colonne_url]:
                if site_nom not in taches_par_site:
                    taches_par_site[site_nom] = []
                
                tache = site_config.copy()
                tache['url'] = row[colonne_url]
                tache['id_set'] = set_id
                tache['nom_set'] = nom_set
                taches_par_site[site_nom].append(tache)
    return taches_par_site

def creer_driver_selenium(scraper_type="standard"):
    """Cr√©e et retourne une instance configur√©e du driver Chrome."""
    logging.info(f"Cr√©ation d'un driver Selenium (type: {scraper_type})")
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    driver = webdriver.Chrome(options=options)

    if scraper_type == "fnac": # Si vous r√©activez la Fnac
        stealth(driver, languages=["fr-FR", "fr"], vendor="Google Inc.", platform="Win32", webgl_vendor="Intel Inc.", renderer="Intel Iris OpenGL Engine", fix_hairline=True)
                
    return driver
    
# Fonction pour envoyer un email d'alerte
def envoyer_email_recapitulatif(baisses_de_prix):
    """
    Prend une liste de baisses de prix et envoie un seul email de r√©sum√©
    """
    
    nombre_baisses = len(baisses_de_prix)
    sujet = f"Alerte Prix LEGO : {nombre_baisses} baisse(s) de prix d√©tect√©e(s) !"
    
    # On cr√©e un email 'alternative' pour avoir une version texte et une version HTML
    msg = MIMEMultipart('alternative')
    msg['Subject'] = sujet
    msg['From'] = EMAIL_ADRESSE
    msg['To'] = EMAIL_DESTINATAIRE

    # On pr√©pare les deux versions du corps de l'email
    text_body = "Bonjour,\n\nVoici les baisses de prix d√©tect√©es aujourd'hui :\n\n"
    html_body = """
    <html>
      <head></head>
      <body style="font-family: sans-serif;">
        <h2>Bonjour,</h2>
        <p>Voici les baisses de prix d√©tect√©es aujourd'hui :</p>
    """
    
    for deal in baisses_de_prix:
        message_affaire = ""
        if deal.get('analyse_affaire') == "tres_bonne":
            message_affaire = "\n   >> C'est une TR√àS bonne affaire üî•üî•"
        elif deal.get('analyse_affaire') == "bonne":
            message_affaire = "\n   >> C'est une bonne affaire ‚úÖ‚úÖ"

        text_body += (
            f"--------------------\n"
            f"Set: {deal['nom_set']}\n"
            f"Site: {deal['site']}\n"
            f"Ancien Prix: {deal['prix_precedent']:.2f}‚Ç¨\n"
            f"NOUVEAU PRIX: {deal['nouveau_prix']:.2f}‚Ç¨{message_affaire}\n"
            f"Lien: {deal['url']}\n"
        )
      
        html_body += f"""
        <hr>
        <div style="padding: 10px;">
            <h3 style="margin-top:0;">{deal['nom_set']}</h3>
            <p style="line-height: 1.5;">
                <b>Site:</b> {deal['site']}<br>
                <b>Ancien Prix:</b> {deal['prix_precedent']:.2f}‚Ç¨<br>
                <b style="color:green; font-size: 1.1em;">NOUVEAU PRIX: {deal['nouveau_prix']:.2f}‚Ç¨</b>
                {message_affaire}
            </p>
            <p><a href="{deal['url']}" style="background-color: #007bff; color: white; padding: 8px 12px; text-decoration: none; border-radius: 5px;">Voir l'offre</a></p>
        </div>
        """
        
    lien_wiki = "https://github.com/Aktawind/lego-price-tracker/wiki"
    text_body += f"\n\nPour une analyse d√©taill√©e, consultez votre tableau de bord : {lien_wiki}"
    html_body += f'<hr><p>Pour une analyse d√©taill√©e et l\'historique des prix, consultez votre <a href="{lien_wiki}">tableau de bord</a>.</p></body></html>'
    
    # On attache les deux versions (texte et HTML)
    msg.attach(MIMEText(text_body, 'plain'))
    msg.attach(MIMEText(html_body, 'html'))
    
    # La partie envoi reste la m√™me
    try:
        with smtplib.SMTP('smtp.gmail.com', 587) as smtp_server:
            smtp_server.starttls()
            smtp_server.login(EMAIL_ADRESSE, EMAIL_MOT_DE_PASSE)
            smtp_server.send_message(msg)
        logging.info(f"Email r√©capitulatif de {nombre_baisses} baisse(s) envoy√© !")
    except Exception as e:
        logging.error(f"Erreur lors de l'envoi de l'email r√©capitulatif : {e}")

def obtenir_localisation_ip():
    """
    Interroge le service ipinfo.io pour conna√Ætre le code pays de l'adresse IP actuelle.
    Retourne le code pays (ex: 'FR', 'US', 'IE') ou None en cas d'erreur.
    """
    try:
        logging.info("R√©cup√©ration de la localisation de l'IP...")
        
        # On fait un appel √† l'API de ipinfo.io qui renvoie du JSON
        reponse = requests.get("https://ipinfo.io/json", timeout=5)
        
        # L√®ve une exception si la requ√™te a √©chou√© (ex: statut 4xx ou 5xx)
        reponse.raise_for_status()
        
        # On convertit la r√©ponse JSON en dictionnaire Python
        data = reponse.json()
        
        # On r√©cup√®re la valeur de la cl√© 'country', avec 'N/A' comme valeur par d√©faut
        pays = data.get('country', 'N/A')
        
        logging.info(f"Localisation d√©tect√©e : Pays={pays}")
        return pays
        
    except requests.exceptions.RequestException as e:
        # G√®re sp√©cifiquement les erreurs de r√©seau (timeout, pas de connexion...)
        logging.error(f"Impossible de contacter le service de localisation IP : {e}")
        return None
    except Exception as e:
        # G√®re toutes les autres erreurs possibles (JSON invalide, etc.)
        logging.error(f"Erreur inattendue lors de la r√©cup√©ration de la localisation de l'IP : {e}")
        return None
    
def verifier_les_prix():
    logging.info("Lancement de la v√©rification des prix")
    
    # Charger les configurations et l'historique
    df_config = charger_configuration_sets_df(FICHIER_CONFIG_EXCEL)
    if df_config is None: return

    try:
        df_historique = pd.read_excel(FICHIER_EXCEL, dtype={'ID_Set': str})
    except FileNotFoundError:
        df_historique = pd.DataFrame(columns=['Date', 'ID_Set', 'Nom_Set', 'Site', 'Prix', 'URL'])
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
        'Accept-Language': 'fr-FR,fr;q=0.9'
    }
    
    lignes_a_ajouter = []
    baisses_de_prix_a_notifier = []
    taches_traitees = set() # "Blacklist" des t√¢ches d√©j√† trait√©es

    # === √âTAPE 1 : TRAITER LES DEALS D'AVENUE DE LA BRIQUE EN PRIORIT√â ===
    logging.info("--- D√©but du traitement des deals d'Avenue de la Brique ---")
    try:
        with open('deals_du_jour.json', 'r', encoding='utf-8') as f:
            deals_avenue = json.load(f)
    except Exception:
        deals_avenue = {}

    for set_id, offres in deals_avenue.items():
        config_set_row_df = df_config.loc[df_config['ID_Set'] == set_id]
        if config_set_row_df.empty: continue
        nom_set = config_set_row_df.iloc[0]['Nom_Set']

        for offre in offres:
            site = offre['site']
            prix_actuel = offre['prix']
            url_offre = offre['url']
            
            logging.info(f"Traitement de '{nom_set}' sur {site} (via Avenue)... Prix actuel : {prix_actuel}‚Ç¨")
            
            df_filtre = df_historique[(df_historique['ID_Set'] == set_id) & (df_historique['Site'] == site)]
            prix_precedent = df_filtre['Prix'].iloc[-1] if not df_filtre.empty else None
            
            if prix_precedent is None or abs(prix_actuel - prix_precedent) > 0.01:
                logging.info(f"Changement de prix d√©tect√© (pr√©c√©dent : {prix_precedent}‚Ç¨).")
                nouvelle_ligne = {'Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ID_Set': set_id, 'Nom_Set': nom_set, 'Site': site, 'Prix': prix_actuel, 'URL': url_offre}
                lignes_a_ajouter.append(nouvelle_ligne)
                
                if prix_precedent is not None and prix_actuel < prix_precedent:
                    logging.info("BAISSE DE PRIX ! Ajout √† la liste de notification.")
                    
                    analyse_affaire = "standard"
                    image_url = ''
                    try:
                        # √âtape 3 : Utiliser le m√™me df_config pour l'analyse
                        config_set_row = df_config.loc[df_config['ID_Set'] == set_id].iloc[0]
                        nb_pieces = pd.to_numeric(config_set_row.get('nbPieces'), errors='coerce')
                        collection = config_set_row.get('Collection', 'default')
                        image_url = config_set_row.get('Image_URL', '')
                        
                        if pd.notna(nb_pieces):
                            prix_moyen = PRIX_MOYEN_PAR_COLLECTION.get(collection, PRIX_MOYEN_PAR_COLLECTION['default'])
                            prix_juste = nb_pieces * prix_moyen
                            if prix_actuel <= prix_juste * SEUIL_TRES_BONNE_AFFAIRE:
                                analyse_affaire = "tres_bonne"
                            elif prix_actuel <= prix_juste * SEUIL_BONNE_AFFAIRE:
                                analyse_affaire = "bonne"
                    except IndexError:
                        logging.warning(f"Infos de config manquantes pour le set {set_id} pour l'analyse de 'bonne affaire'.")

                    baisses_de_prix_a_notifier.append({
                        'nom_set': nom_set, 'nouveau_prix': prix_actuel,
                        'prix_precedent': prix_precedent, 'site': site, 'url': url_offre,
                        'image_url': image_url, 'analyse_affaire': analyse_affaire
                    })
            else:
                logging.info("Pas de changement de prix.")
            time.sleep(5)

            # On marque cette t√¢che comme "faite" pour ne pas la re-scraper
            taches_traitees.add((set_id, site))

    # === √âTAPE 2 : TRAITER LES T√ÇCHES MANUELLES RESTANTES ===
    taches_manuelles = regrouper_taches_par_site(df_config)
    SCRAPERS = { "amazon": scrapers.scrape_amazon, "carrefour": scrapers.scrape_carrefour, "standard": scrapers.scrape_standard }

    for site, taches in taches_manuelles.items():
        logging.info(f"--- D√©but du traitement manuel pour : {site} ---")
        
        # On filtre les t√¢ches pour ne garder que celles qui n'ont pas √©t√© trait√©es
        taches_a_faire = [t for t in taches if (t['id_set'], site) not in taches_traitees]
        
        if not taches_a_faire:
            logging.info(f"Toutes les t√¢ches pour {site} ont d√©j√† √©t√© trait√©es via Avenue de la Brique. On ignore.")
            continue

        site_config = CONFIG_SITES.get(site)
        if not site_config: continue
        
        scraper_type = site_config.get('type')
        scraper_function = SCRAPERS.get(scraper_type)
        if not scraper_function: continue

        driver = None
        if site_config.get("use_selenium", False):
            try:
                driver = creer_driver_selenium(scraper_type)
                if scraper_type == "amazon":
                    pays_actuel = obtenir_localisation_ip()
                    if pays_actuel and pays_actuel != 'FR':
                        logging.info(f"IP non-fran√ßaise ({pays_actuel}) d√©tect√©e. For√ßage de la localisation pour Amazon...")
                        try:
                            driver.get("https://www.amazon.fr/")
                            wait = WebDriverWait(driver, 10)
                            
                            # === D√âBUT DE LA MODIFICATION ===

                            # 1. On g√®re les cookies sur la page d'accueil AVANT tout le reste
                            try:
                                bouton_cookies = wait.until(EC.element_to_be_clickable((By.ID, "sp-cc-accept")))
                                bouton_cookies.click()
                                logging.info("  -> Banni√®re de cookies sur la page d'accueil g√©r√©e.")
                                time.sleep(1) # Petite pause pour laisser la banni√®re dispara√Ætre
                            except Exception:
                                logging.info("  -> Pas de banni√®re de cookies sur la page d'accueil.")

                            # 2. On utilise un s√©lecteur plus robuste pour le bouton de localisation
                            #    On cherche un lien ou un div qui a un ID contenant "location"
                            xpath_localisation = "//*[@id='nav-global-location-popover-link' or @id='glow-ingress-block']"
                            bouton_localisation = wait.until(
                                EC.element_to_be_clickable((By.XPATH, xpath_localisation))
                            )
                            bouton_localisation.click()
                            
                            # 3. Le reste est inchang√© car vos nouveaux extraits HTML le confirment
                            champ_postal = wait.until(EC.visibility_of_element_located((By.ID, "GLUXZipUpdateInput")))
                            champ_postal.clear() # On vide le champ au cas o√π il serait pr√©-rempli
                            champ_postal.send_keys("38540")
                            
                            bouton_actualiser_container = wait.until(EC.element_to_be_clickable((By.ID, "GLUXZipUpdate")))
                            bouton_actualiser_container.click()
                            
                            # 4. On attend que la page se recharge en v√©rifiant que le code postal est bien mis √† jour
                            wait.until(EC.text_to_be_present_in_element((By.ID, "glow-ingress-line2"), "38540"))
                            logging.info("Localisation fran√ßaise pour Amazon forc√©e avec succ√®s.")  
                            
                        except Exception as e:
                            # Si la localisation √©choue, c'est une erreur critique pour Amazon
                            logging.error(f"La proc√©dure de for√ßage de localisation pour Amazon a √©chou√© : {e}")
                            driver.quit() # On ferme le driver
                            continue # ON PASSE AU SITE SUIVANT
                            
                    else:
                        logging.info("IP fran√ßaise (ou non d√©tect√©e), pas de for√ßage n√©cessaire pour Amazon.")
                
            except Exception as e:
                logging.error(f"Impossible de d√©marrer/pr√©parer Selenium pour {site}: {e}")
                if driver: driver.quit()
                continue

        # Boucle secondaire : on traite chaque produit
        for tache in taches_a_faire:
            logging.info(f"V√©rification de '{tache['nom_set']}'...")
            prix_actuel = None

             # On r√©cup√®re l'URL brute et on la nettoie syst√©matiquement
            url_brute = tache['url']
            url_propre = url_brute.strip().rstrip(':/')
            
            # On v√©rifie si l'URL a √©t√© modifi√©e pour le log
            if url_brute != url_propre:
                logging.info(f"  -> URL nettoy√©e : de '{url_brute}' √† '{url_propre}'")
            
            try:
                kwargs = {'url': tache['url']}
                if driver:
                    kwargs['driver'] = driver
                else:
                    kwargs['headers'] = headers
                
                if 'selecteur' in tache and tache['selecteur']:
                    if isinstance(tache['selecteur'], dict):
                        kwargs.update(tache['selecteur'])
                    else:
                        kwargs['selecteur'] = tache['selecteur']
                
                prix_actuel = scraper_function(**kwargs)
            except Exception as e:
                logging.error(f"Erreur inattendue lors de l'appel du scraper pour {tache['url']}: {e}")

            if prix_actuel is None:
                logging.warning("Prix non trouv√©.")
                continue
            
            logging.info(f"Prix actuel : {prix_actuel}‚Ç¨")
            df_filtre = df_historique[(df_historique['ID_Set'] == tache['id_set']) & (df_historique['Site'] == site)]
            prix_precedent = df_filtre['Prix'].iloc[-1] if not df_filtre.empty else None
            
            if prix_precedent is None or abs(prix_actuel - prix_precedent) > 0.01:
                logging.info(f"Changement de prix d√©tect√© (pr√©c√©dent : {prix_precedent}‚Ç¨). Enregistrement...")
                nouvelle_ligne = {'Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ID_Set': tache['id_set'], 'Nom_Set': tache['nom_set'], 'Site': site, 'Prix': prix_actuel, 'URL': url_propre}
                lignes_a_ajouter.append(nouvelle_ligne)
                
                if prix_precedent is not None and prix_actuel < prix_precedent:
                    logging.info("BAISSE DE PRIX ! Ajout √† la liste de notification.")
                    
                    analyse_affaire = "standard"
                    image_url = ''
                    try:
                        # √âtape 3 : Utiliser le m√™me df_config pour l'analyse
                        config_set_row = df_config.loc[df_config['ID_Set'] == tache['id_set']].iloc[0]
                        nb_pieces = pd.to_numeric(config_set_row.get('nbPieces'), errors='coerce')
                        collection = config_set_row.get('Collection', 'default')
                        image_url = config_set_row.get('Image_URL', '')
                        
                        if pd.notna(nb_pieces):
                            prix_moyen = PRIX_MOYEN_PAR_COLLECTION.get(collection, PRIX_MOYEN_PAR_COLLECTION['default'])
                            prix_juste = nb_pieces * prix_moyen
                            if prix_actuel <= prix_juste * SEUIL_TRES_BONNE_AFFAIRE:
                                analyse_affaire = "tres_bonne"
                            elif prix_actuel <= prix_juste * SEUIL_BONNE_AFFAIRE:
                                analyse_affaire = "bonne"
                    except IndexError:
                        logging.warning(f"Infos de config manquantes pour le set {tache['id_set']} pour l'analyse.")

                    baisses_de_prix_a_notifier.append({
                        'nom_set': tache['nom_set'], 'nouveau_prix': prix_actuel,
                        'prix_precedent': prix_precedent, 'site': site, 'url': url_offre,
                        'image_url': image_url, 'analyse_affaire': analyse_affaire
                    })
            else:
                logging.info("Pas de changement de prix.")
            time.sleep(5)
        
        if driver:
            logging.info(f"Fermeture de la session Selenium pour {site}")
            driver.quit()

    # === √âTAPE 3 : NOTIFICATION ET SAUVEGARDE FINALES ===
    if baisses_de_prix_a_notifier:
        envoyer_email_recapitulatif(baisses_de_prix_a_notifier)
    if lignes_a_ajouter:
        df_a_ajouter = pd.DataFrame(lignes_a_ajouter)
        df_historique = pd.concat([df_historique, df_a_ajouter], ignore_index=True)
        df_historique.to_excel(FICHIER_EXCEL, index=False)
        logging.info(f"{len(lignes_a_ajouter)} modifications enregistr√©es dans le fichier Excel.")

# --- POINT D'ENTR√âE ---
if __name__ == "__main__":
    if not all([EMAIL_ADRESSE, EMAIL_MOT_DE_PASSE, EMAIL_DESTINATAIRE]):
        logging.error("Variables d'environnement pour l'email non configur√©es. Arr√™t.")
    else:
        verifier_les_prix()